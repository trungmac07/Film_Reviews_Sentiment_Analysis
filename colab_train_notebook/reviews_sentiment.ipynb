{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"glove/glove.6B.50d.txt\"\n",
    "\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embedded_words = {}\n",
    "    embedded_words[' '] = np.array([0] * 50, dtype='float32')\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedded_words[word] = vector\n",
    "\n",
    "    \n",
    "    return embedded_words\n",
    "\n",
    "embedded_words = load_glove_embeddings(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_index = {word:index for index, word in enumerate(sorted(embedded_words.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['review'].tolist()\n",
    "labels = df['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"./stopwords\")\n",
    "nltk.download('stopwords', download_dir=\"./stopwords\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def sentence_to_index_tokens(sentence, words_index, max_len = 15000):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[\\']', ' ', sentence)\n",
    "    sentence = re.sub(r'<[^>]{0,5}>|[^a-zA-Z\\s]', ' ', sentence)\n",
    "    tokens = sentence.split()\n",
    "    tokens = [words_index[token] for token in tokens if token not in stop_words and token in words_index]\n",
    "    if(len(tokens) < max_len):\n",
    "        tokens += [0] * (max_len - len(tokens))\n",
    "    return tokens[:max_len]\n",
    "\n",
    "def preprocess_training_data(sentences, labels, words_index, max_len = 15000):\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for i,(sentence,label) in enumerate(zip(sentences,labels)):\n",
    "        tokens = sentence_to_index_tokens(sentence, words_index, max_len=max_len)\n",
    "        preprocessed_data.append((torch.tensor(tokens),torch.tensor(label)))\n",
    "    \n",
    "    return preprocessed_data\n",
    "        \n",
    "MAX_LEN = 1000\n",
    "data = preprocess_training_data(sentences, labels, words_index, max_len=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedded_words, words_index):\n",
    "    embedding_matrix = np.zeros((len(words_index), 50), dtype='float32')\n",
    "    for word, i in words_index.items():\n",
    "        embedding_vector = embedded_words.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    \n",
    "    return torch.tensor(embedding_matrix)\n",
    "    \n",
    "embedding_matrix = create_embedding_matrix(embedded_words, words_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.dtype)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(0.8 * len(data))\n",
    "test_len = len(data) - train_len\n",
    "BATCH_SIZE = 32\n",
    "train, test = random_split(data, [train_len, test_len], generator=torch.Generator().manual_seed(77))\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(400001,50)\n",
    "embedding_layer.weight = nn.Parameter(embedding_matrix)\n",
    "embedding_layer.weight.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size = 400001,\n",
    "                embedding_dim = 50,\n",
    "                hidden_dim = 256,\n",
    "                output_dim = 1, \n",
    "                n_layers = 2, \n",
    "                bidirectional = True, \n",
    "                dropout = 0.3,\n",
    "                embedding_matrix = None,\n",
    "                batch_first = True,\n",
    "                device = 'cpu'):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_dim, device=device)\n",
    "        \n",
    "        if(embedding_matrix is not None):\n",
    "            self.embedding.weight = nn.Parameter(embedding_matrix.to(device), requires_grad=False)\n",
    "\n",
    "        self.rnn=nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, device=device, batch_first=batch_first)\n",
    "\n",
    "        if(bidirectional):\n",
    "            self.fc = nn.Linear(hidden_dim*2, output_dim, device=device)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim, device=device)\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.embedding(x)\n",
    "        lstm_out,(hidden,cell)=self.rnn(out)\n",
    "        if(self.bidirectional):\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else: \n",
    "            hidden = hidden[-1,:,:]\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        out=self.fc(hidden.squeeze(0))\n",
    "        \n",
    "        return out.squeeze()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "def fit(model, data, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)#,lr=0.001, betas=(0.9,0.999))\n",
    "    EPOCHS = 10\n",
    "    model.train()\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for i, (x_batch,y_batch) in enumerate(data):\n",
    "            x = Variable(x_batch).to(device)\n",
    "            y = Variable(y_batch).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model.forward(x)\n",
    "\n",
    "            loss = model.loss(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = torch.round(F.sigmoid(y_pred))\n",
    "  \n",
    "            correct += (predicted == y).sum()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\"{:<15} {:<15} {:<30} {:<30}\".format(\"Epoch: \" + str(e), \"| Batch: \" + str(i), \"| Loss: \" + str(loss.item()), \"| accuracy: \" + str(float(correct/float(BATCH_SIZE*(i+1))))))\n",
    "        if((e+1) % 5 == 0):\n",
    "            torch.save(model.state_dict(), 'lstm-'+str(e+1)+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm = LSTMModel(n_layers=2, embedding_matrix=embedding_matrix, device = device)\n",
    "print(\"Training on\",device)\n",
    "fit(lstm, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMModel(n_layers=2, embedding_matrix=embedding_matrix, device = device)\n",
    "lstm.load_state_dict(torch.load(\"./lstm-10.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test, device = 'cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    for i, (x,y) in enumerate(test):\n",
    "        x = Variable(x).to(device)\n",
    "        y = Variable(y).float().to(device)\n",
    "        y_pred = model.forward(x)\n",
    "        predicted = torch.round(F.sigmoid(y_pred))\n",
    "        n += BATCH_SIZE\n",
    "        correct +=  (predicted == y).sum().item()\n",
    "            \n",
    "    return correct/n \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval(lstm, test_loader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Today I'm really exicited about this. I love it. In my whole time in the cinema, I can't stop laughing. This is the best movie I have ever seen. Thanks\"\n",
    "s = sentence_to_index_tokens(a,words_index)\n",
    "\n",
    "prob = F.sigmoid(lstm(torch.tensor([s], device=device)))\n",
    "if(prob >= 0.5):\n",
    "    print(\"Good:\", prob.item())\n",
    "else:\n",
    "    print(\"Bad:\", 1 - prob.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
